{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b4e48c258951a80",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">TF-IDF Assignment</h1>\n",
    "<h2 style=\"text-align: center;\">November 30, 2025</h2>\n",
    "<h3 style=\"text-align: center;\">Yucheng Bao</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde328176bc53af",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Becoming self-regulated benefits students in multiple ways during their learning process. Therein, the regulation of motivation has been identified as an essential component of self-regulated learning, as motivation is associated with academic persistence, effort-making, and performance (Zimmerman & Schunk, 2012). To understand the strategies students use in relation to their motivation, researchers have developed survey methods (Wolters, 1999; Schwinger et al., 2009). For instance, students may be asked to rate the likelihood or frequency of use of described strategies in hypothetical learning situations or through their retrospective learning reflections, using a Likert scale. However, researchers note that the use of closed-ended (Likert-scale, forced-choice) questions can lead to biased responses because it prevents respondents from describing the strategies they use freely (Zepeda & Nokes-Malach, 2021). Given this limitation, Mottett et al. (in prep) examine 146 students’ open-ended responses regarding their motivational experiences and manually categorize the motivational strategies mentioned in the responses using an inductive, qualitative thematic coding method. The results demonstrate that while students employed a variety of strategies covered in existing questionnaires, they also used strategies not included (e.g., motivating others and fear of failure), which reflects the advantage of qualitative approaches in capturing nuances in students’ thoughts and behaviors.<br>\n",
    "\n",
    "A coin has two sides. Although qualitative coding provides rich insights into students’ motivational experiences, it requires extensive time and human resources, making it impractical for large databases with thousands of samples. In contrast, natural language processing (NLP) offers significant advantages, as it possesses the capability to analyze extensive textual datasets while uncovering intricate patterns efficiently (Gamieldien et al., 2023). Research has shown the utility of NLP approaches for conducting traditional thematic analysis using Topic Modeling methods such as Term Frequency–Inverse Document Frequency (TF-IDF) and Word2Vec. This study aims to extend the application of NLP to the education field, specifically in the learning setting of a large-scale digital learning platform designed for middle school math education (6-8th grades).<br>\n",
    "\n",
    "This study aims to evaluate students' use of motivational strategies via an NLP approach: Most previous research on students' motivational regulation has focused on college student groups (Miele et al., in prep). This study extends the topic to the high school learning context, utilizing a qualitative inquiry approach, and serves as an early academic work, contributing evidence from other student groups. In addition, by comparing TF-IDF–based NLP outputs with human-coded themes, this study examines the extent to which automated analyses align with human qualitative judgments, offering insights into NLP's potential as a complementary tool for motivational research.\n",
    "\n",
    "### Research Questions\n",
    "**Research Question 1**: To what extent can a machine-learning model using the full TF-IDF feature matrix predict human-coded motivational strategies in students’ open-ended reflections?<br>\n",
    "**Research Question 2**: Building up to the first question, to what extent can a machine-learning model using the top TF-IDF-derived predictive keywords (positive and negative) predict the same human-coded motivational strategies?\n",
    "\n",
    "### Hypothese\n",
    "**Hypothesis 1**: A machine-learning model trained on the full TF-IDF feature matrix will significantly predict the human-coded motivational strategies in students’ open-ended reflections.<br>\n",
    "**Hypothesis 2**: A machine-learning model trained on the top TF-IDF–derived predictive keywords (positive and negative) will also predict the human-coded motivational strategies, although with lower accuracy than the full TF-IDF model.<br>\n",
    "**Hypothesis 3**: TF-IDF-based NLP models will show above-chance ability to predict human-coded motivational regulation strategies. However, despite their predictive accuracy, the top TF-IDF features are unlikely to map neatly onto the conceptual definitions derived through human coding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8711542ab1fc70f7",
   "metadata": {},
   "source": [
    "### Data\n",
    "This study is part of a larger project involving 5404 middle school students (Grades 6-8) enrolled in MATHia, a digital Math Learning platform that serves as complementary instructional support. Due to the data privacy agreement and the scope of the research interests, the demographic information for the school districts, schools, teachers, and students involved in this study is not disclosed. Students followed teachers’ instructions to engage with instructional materials in MATHia. After the overall learning sessions, students were prompted to respond to an open-ended question: “_As you were working on this content, how did you motivate yourself? Please describe in as much detail as you can._”<br>\n",
    "\n",
    "An open-ended question coding team comprised two research assistants (RAs) and the PIs. First, the two RAs read through a random sample of 30% of the total data and individually categorized student responses into different strategies following the coding protocol developed by Mottett et al. (in prep). Specifically, 0 indicates that the response does not reflect a particular motivational strategy, and 1 indicates that the response explicitly demonstrates or mentions one strategy. The two RAs then discussed and compared their categories, resolving any discrepancies that emerged. If an unlabeled motivational strategy repeatedly appeared during this initial review, the RAs consulted with the PIs to determine whether the pattern reflected a conceptually distinct theme. Together, the team engaged in iterative discussions to define, refine, and formalize this new category before proceeding with full dataset coding. From there, they developed a sample-adaptive protocol that included explanations of the strategies and example responses. The two RAs then coded another random sample used to develop the protocol (30% of the data). Using this refined protocol, the two RAs coded another random 30% sample. Once interrater reliability reached acceptable thresholds (Cohen’s κ > .70 and >90% agreement), the RAs independently coded the remaining responses.<br>\n",
    "\n",
    "At the time of analysis, the whole coding process was still ongoing. Therefore, the present study focuses on a subsample of student responses (N = 764) that had already been fully coded according to the finalized coding protocol. Meanwhile, I conducted an additional manual inspection of the coded dataset to examine the frequency distribution of each motivational strategy. Based on this inspection, the current analysis focuses on the three most frequently observed codes: Willpower, Performance Self-Talk, and Effort, as these strategies appeared most frequently and provided a strong foundation for modeling students’ motivational patterns. A condensed coding protocol is provided below.\n",
    "\n",
    "<img src=\"TF-IDF Assignment_files/images/Coding_Rubric.png\" alt=\"Results for Performance Self-talk\" width=\"800\">\n",
    "\n",
    "#### Data Cleaning and Preprocessing\n",
    "All open-ended responses were processed using a multi-stage text-cleaning pipeline designed to remove noise, standardize linguistic structure, and prepare the data for subsequent natural language processing (NLP) analyses.\n",
    "##### _Text Normalization_\n",
    "We first standardized the raw text input to ensure uniform formatting across responses. All entries were converted to strings and converted to lowercase. Non-alphabetic characters, such as numbers, punctuation, and symbols, were removed by using regular expressions. Responses containing fewer than three characters were excluded at this stage, as they are more likely not to contain interpretable linguistic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c9fcb27367215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "# Change to the working directory\n",
    "os.chdir('/Users/zepedalab/Desktop/MATHia_NLP_YB')\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db970267030296b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:50:00.370470Z",
     "start_time": "2025-12-01T01:49:59.353035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Data File\n",
    "text_df = pd.read_excel('/Users/zepedalab/Desktop/MATHia_NLP_YB/E1C3 Codes + partial E2C3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30cc42c3ee6ac7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:50:16.642444Z",
     "start_time": "2025-12-01T01:50:16.625186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-process the text data (text_df)\n",
    "# Convert the text input to the lower case string\n",
    "# Remove all non-alphabetic characters\n",
    "text_df['input'] = text_df['input'].astype(str)\n",
    "text_df['input'] = text_df['input'].str.lower()\n",
    "text_df['input'] = text_df['input'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "# Filter out any input that comprise less than 3 characters\n",
    "text_df = text_df[text_df['input'].str.len() >= 3]\n",
    "short_input_df = text_df[text_df['input'].str.len() < 3] #short_input_df allows us to manually check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca429fafcebe2b1e",
   "metadata": {},
   "source": [
    "##### _Nonsense String Detection and Removal_\n",
    "During the initial manual review of students’ open-ended responses, we observed a substantial number of entries that were nonsensical or nonlinguistic (such as random characters, repeated letters, or strings lacking semantic content), which required further preprocessing. To address this issue, I implemented a two-step noise-removal procedure designed to identify and remove such unusable inputs. First, the nostril nonsense detector was applied to evaluate whether a text contains irregular or nonlinguistic character patterns (Hucka, 2018). All responses that were classified as nonsense were excluded. As the nostril nonsense detector requires a minimum string length of 6 characters for classification, some short strings cannot be evaluated. I then used a trained gibberish-detection model that imposes no length requirement on the remaining responses that had passed the nostril nonsense detector (Neuhaus & Ruvinskiy, 2015). Any input classified as gibberish was removed. While both tools have limitations and may not perfectly detect all forms of nonlinguistic input, using them in combination allowed us to remove the majority of uninterpretable responses and retain, to the greatest extent possible, linguistically meaningful texts suitable for subsequent NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d69064369643b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:50:42.732952Z",
     "start_time": "2025-12-01T01:50:42.049714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Detecting the nonsense string\n",
    "from nostril import nonsense\n",
    "short_indices = []\n",
    "nonsense_indices = []\n",
    "\n",
    "# Iterate through rows and separate nonsense rows\n",
    "for index, row in text_df.iterrows():\n",
    "    try:\n",
    "        text = row['input']\n",
    "        if len(text) < 6: # Check if the text is too short to test\n",
    "            # Add the short text to short_text_df\n",
    "            if len(text) < 6:\n",
    "                short_indices.append(index)\n",
    "            continue\n",
    "\n",
    "        # Use nostril to detect nonsense in the current row's text\n",
    "        if nonsense(text):\n",
    "            nonsense_indices.append(index)\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        if \"Text is too short to test\" in msg:\n",
    "             short_indices.append(index)\n",
    "        else:\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "\n",
    "short_text_df = text_df.loc[short_indices]\n",
    "nonsense_df = text_df.loc[nonsense_indices]\n",
    "text_df = text_df.drop(nonsense_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "886bf4c8e8ab542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:14:53.069734Z",
     "start_time": "2025-12-01T02:14:53.058279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4                                  jjjnnknjmimkkikiilkin\n",
       "99                                           ummmmmm idk\n",
       "103    jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj...\n",
       "159                                              yayyyyy\n",
       "162    dddddddddddddddddddddddddddddddddddddddddddddd...\n",
       "263                            iiiiiiiiiiiiiiiiiiiiii kj\n",
       "359                                           v guvghibj\n",
       "463                                             ndcfksdj\n",
       "480                                   ibhuiygutgcytuyvjh\n",
       "505                                               jkkkkk\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quicklook on the nonsense string\n",
    "nonsense_df.input[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f796b43fbfdd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:50:58.762896Z",
     "start_time": "2025-12-01T01:50:58.707983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-process those shor input by using the gibberish detector\n",
    "from gibberish_detector import detector\n",
    "Detector = detector.create_from_model('/Users/zepedalab/PycharmProjects/MATHia-NLP-YB/gibberish-detector.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d67f96cef49559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:51:05.122823Z",
     "start_time": "2025-12-01T01:51:05.094661Z"
    }
   },
   "outputs": [],
   "source": [
    "gibberish_indices = []\n",
    "\n",
    "# Iterate through rows and separate gibberish rows\n",
    "for index, row in text_df.iterrows():\n",
    "    try:\n",
    "        # Check if the row is gibberish\n",
    "        is_gibberish = Detector.is_gibberish(row['input'])\n",
    "\n",
    "        # If it is gibberish, add to the gibberish_df and remove from text_df_3\n",
    "        if is_gibberish:\n",
    "            gibberish_indices.append(index)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "\n",
    "gibberish_df = text_df.loc[gibberish_indices]\n",
    "text_df = text_df.drop(gibberish_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "272b30d025d282ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:14:48.912090Z",
     "start_time": "2025-12-01T02:14:48.899959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93      bruhhhh\n",
       "111    r gtktnq\n",
       "113         idk\n",
       "190         idk\n",
       "191         idk\n",
       "193         kjh\n",
       "213        kklj\n",
       "246         idk\n",
       "254        jhgj\n",
       "258         idk\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quicklook on the gibberish string\n",
    "gibberish_df.input[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83585a27c503d600",
   "metadata": {},
   "source": [
    "##### _Linguistic Preprocessing_\n",
    "After removing nonsensical and gibberish responses, we conducted a series of linguistic preprocessing steps to standardize the textual data and prepare it for downstream natural language processing analyses. All remaining responses were first tokenized using Gensim’s tokenizer, which segments each response into individual tokens. Each token was then lemmatized with the WordNetLemmatizer from the Natural Language Toolkit (NLTK) to reduce inflected forms to their base lemma (e.g., studying, studied, and studies → study), ensuring that semantically similar words were represented consistently. Following lemmatization, English stopwords (e.g., the, and, but) were removed using the NLTK stopword corpus to eliminate high-frequency function words that do not meaningfully contribute to semantic content. Only alphabetic tokens were retained, and all tokens containing numbers, symbols, or mixed-character patterns were removed. Because this preprocessing pipeline occasionally produced responses with no remaining valid tokens (e.g., responses consisting solely of stopwords or removed characters), we excluded entries whose token lists were empty after processing. This procedure ensured that the final dataset consisted exclusively of responses containing at least one linguistically interpretable lemma suitable for feature extraction and subsequent modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df72d2aa91ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages for the later NLP analysis\n",
    "import gensim\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download the wordnet resource if not already done\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5c3c5dc07e5fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:51:39.583557Z",
     "start_time": "2025-12-01T01:51:37.635962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Tokenize and lemmatize the documents\n",
    "tokenized_docs = [\n",
    "    [lemmatizer.lemmatize(word.lower()) for word in gensim.utils.tokenize(doc)\n",
    "     if word.lower() not in stop_words and word.isalpha()]\n",
    "    for doc in text_df.input\n",
    "    if len([word for word in gensim.utils.tokenize(doc) if word.lower() not in stop_words and word.isalpha()]) > 0\n",
    "]\n",
    "\n",
    "# Create filtered_docs: Only keep non-empty documents\n",
    "filtered_docs = [doc for doc in tokenized_docs if len(doc) > 0]\n",
    "\n",
    "# Filter text_df to remove rows that correspond to empty tokenized documents\n",
    "# Ensure that text_df_filtered matches filtered_docs in length\n",
    "text_df_filtered = text_df.iloc[:len(filtered_docs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cadf96adfb2e14",
   "metadata": {},
   "source": [
    "#### TF-IDF Analysis\n",
    "I first generated a bag-of-words matrix in which each response was encoded as counts of all unique lemmatized tokens. Term frequency (TF) was then computed by dividing each token count by the total number of tokens within the response. Next, I calculated inverse document frequency (IDF) for each word as the logarithm of the total number of responses divided by the number of responses in which the word appeared. The final TF–IDF matrix was obtained by multiplying the TF matrix by the IDF values, producing a weighted representation, in which words that were distinctive to individual responses received higher scores, while commonly used words received lower scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f3b6a05de15ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:52:00.928314Z",
     "start_time": "2025-12-01T01:52:00.917590Z"
    }
   },
   "outputs": [],
   "source": [
    "# create empty dictionary from gensim library (for the vocabulary)\n",
    "dictionary = gensim.corpora.Dictionary()\n",
    "# iteratively add each doc to the bag-of-words corpus\n",
    "bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in filtered_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4a06c35b3998c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:14:42.246036Z",
     "start_time": "2025-12-01T02:14:42.154789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awaly</th>\n",
       "      <th>better</th>\n",
       "      <th>last</th>\n",
       "      <th>time</th>\n",
       "      <th>nan</th>\n",
       "      <th>ask</th>\n",
       "      <th>class</th>\n",
       "      <th>classmate</th>\n",
       "      <th>fellow</th>\n",
       "      <th>focus</th>\n",
       "      <th>...</th>\n",
       "      <th>explaining</th>\n",
       "      <th>enough</th>\n",
       "      <th>pay</th>\n",
       "      <th>sucessful</th>\n",
       "      <th>belving</th>\n",
       "      <th>motavited</th>\n",
       "      <th>unit</th>\n",
       "      <th>went</th>\n",
       "      <th>push</th>\n",
       "      <th>continue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 676 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   awaly  better  last  time  nan  ask  class  classmate  fellow  focus  ...  \\\n",
       "0    1.0     1.0   1.0   1.0  0.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "1    0.0     0.0   0.0   0.0  1.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "2    0.0     0.0   0.0   0.0  1.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "3    0.0     0.0   0.0   0.0  1.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "4    0.0     0.0   0.0   0.0  0.0  2.0    1.0        1.0     1.0    1.0  ...   \n",
       "5    0.0     0.0   0.0   0.0  0.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "6    0.0     0.0   0.0   0.0  0.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "7    0.0     0.0   0.0   0.0  0.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "8    0.0     0.0   0.0   0.0  0.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "9    0.0     0.0   0.0   0.0  0.0  0.0    0.0        0.0     0.0    0.0  ...   \n",
       "\n",
       "   explaining  enough  pay  sucessful  belving  motavited  unit  went  push  \\\n",
       "0         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "1         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "2         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "3         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "4         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "5         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "6         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "7         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "8         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "9         0.0     0.0  0.0        0.0      0.0        0.0   0.0   0.0   0.0   \n",
       "\n",
       "   continue  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  \n",
       "5       0.0  \n",
       "6       0.0  \n",
       "7       0.0  \n",
       "8       0.0  \n",
       "9       0.0  \n",
       "\n",
       "[10 rows x 676 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get vocab from dictionary\n",
    "vocab = pd.Series(dictionary.token2id).index\n",
    "\n",
    "# Sequence of numbers as long as the documents\n",
    "index = range(len(bow_corpus))\n",
    "\n",
    "# Create a pandas dataframe full of zeros.\n",
    "bow_df = pd.DataFrame(data=np.zeros((len(bow_corpus), len(dictionary)), dtype=np.float32),\n",
    "                  index=index, # row labels\n",
    "                  columns=vocab) #column labels\n",
    "\n",
    "# Add each word from the bag-of-words corpus to the matrix.\n",
    "for idx in index:\n",
    "  for id, freq in bow_corpus[idx]:\n",
    "    bow_df.loc[idx,dictionary[id]] = freq\n",
    "\n",
    "# Quick view on bag of words metrix\n",
    "bow_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "624d01dbe5797025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:14:36.301565Z",
     "start_time": "2025-12-01T02:14:36.053332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awaly</th>\n",
       "      <th>better</th>\n",
       "      <th>last</th>\n",
       "      <th>time</th>\n",
       "      <th>nan</th>\n",
       "      <th>ask</th>\n",
       "      <th>class</th>\n",
       "      <th>classmate</th>\n",
       "      <th>fellow</th>\n",
       "      <th>focus</th>\n",
       "      <th>...</th>\n",
       "      <th>explaining</th>\n",
       "      <th>enough</th>\n",
       "      <th>pay</th>\n",
       "      <th>sucessful</th>\n",
       "      <th>belving</th>\n",
       "      <th>motavited</th>\n",
       "      <th>unit</th>\n",
       "      <th>went</th>\n",
       "      <th>push</th>\n",
       "      <th>continue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.584206</td>\n",
       "      <td>1.583819</td>\n",
       "      <td>1.583911</td>\n",
       "      <td>1.583804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.335073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.335073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.335073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.407856</td>\n",
       "      <td>0.703923</td>\n",
       "      <td>0.703993</td>\n",
       "      <td>0.704092</td>\n",
       "      <td>0.703913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 676 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      awaly    better      last      time       nan       ask     class  \\\n",
       "0  1.584206  1.583819  1.583911  1.583804  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  6.335073  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  6.335073  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  6.335073  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  1.407856  0.703923   \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   classmate    fellow     focus  ...  explaining  enough  pay  sucessful  \\\n",
       "0   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "1   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "2   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "3   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "4   0.703993  0.704092  0.703913  ...         0.0     0.0  0.0        0.0   \n",
       "5   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "6   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "7   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "8   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "9   0.000000  0.000000  0.000000  ...         0.0     0.0  0.0        0.0   \n",
       "\n",
       "   belving  motavited  unit  went  push  continue  \n",
       "0      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "1      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "2      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "3      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "4      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "5      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "6      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "7      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "8      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "9      0.0        0.0   0.0   0.0   0.0       0.0  \n",
       "\n",
       "[10 rows x 676 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating Inverse Document Frequency\n",
    "num_documents = len(bow_df) #get the length of the documents\n",
    "\n",
    "# function of idf\n",
    "def idf(term):\n",
    "  documents_containing_term = (bow_df[term] > 0).sum()    # sum of documents containing the term\n",
    "  return np.log(num_documents+1/documents_containing_term+1)  # divide the number of documents by the num documents with the term and get the log.\n",
    "\n",
    "# dictionary comprehension to apply the idf function to every word (column) in the bow_df DataFrame\n",
    "idf = pd.Series({term:idf(term) for term in bow_df.columns}) # create dictionary with key value pair of term:idf(term)\n",
    "\n",
    "#Calculate Term Frequency\n",
    "# divide each token count by the total token count for the row.\n",
    "tf_df = bow_df.div(bow_df.sum(axis='columns'), axis='rows')\n",
    "\n",
    "#Calculate the TF-IDF score\n",
    "tfidf_df = tf_df.copy().multiply(idf, axis='columns')\n",
    "# make copy of tf_df that is tf/idf\n",
    "\n",
    "# Quick view on tf-idf metrix\n",
    "tfidf_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16785b37df0067",
   "metadata": {},
   "source": [
    "To examine whether students’ open-ended responses contained linguistic cues predictive of a specific human-coded motivational strategy, I trained a logistic regression classifier using the TF–IDF matrix as input features. Because the distribution of selected code (i.e., Willpower, Performance Self-talk, Effort) was imbalanced (more instances labeled as 0 than 1), I constructed a balanced training dataset by identifying the minority class and randomly sampling an equal number of responses from the majority class (with a fixed random seed to ensure reproducibility). The resulting subset contained equal numbers of responses labeled 0 and 1. This balanced dataset was then randomly shuffled prior to training.\n",
    "\n",
    "A logistic regression classifier was trained and fitted to the balanced TF–IDF features. Model performance was evaluated using predictions made on the same balanced dataset, and standard classification metrics were computed, including accuracy, precision, recall, and the full classification report. A confusion matrix was also generated to summarize the model’s performance across the two classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4697484c2600e682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T01:59:08.249670Z",
     "start_time": "2025-12-01T01:59:07.873045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Willpower\n",
      "0    459\n",
      "1    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced class counts:\n",
      "Willpower\n",
      "1    104\n",
      "0    104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classes in classifier: [0 1]\n",
      "\n",
      "Logistic Regression Accuracy (Balanced Data): 0.8846153846153846\n",
      "Logistic Regression Precision (Balanced Data): 0.8389830508474576\n",
      "Logistic Regression Recall (Balanced Data): 0.9519230769230769\n",
      "\n",
      "Classification Report (Balanced Data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.82      0.88       104\n",
      "           1       0.84      0.95      0.89       104\n",
      "\n",
      "    accuracy                           0.88       208\n",
      "   macro avg       0.89      0.88      0.88       208\n",
      "weighted avg       0.89      0.88      0.88       208\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Select the target human code\n",
    "y_raw = text_df_filtered['Willpower']\n",
    "\n",
    "# Make sure X (tfidf_df) and y share the same indices\n",
    "valid_idx = tfidf_df.index.intersection(y_raw.index)\n",
    "\n",
    "X = tfidf_df.loc[valid_idx]\n",
    "y = y_raw.loc[valid_idx]\n",
    "print(y.value_counts())\n",
    "\n",
    "# Build a balanced sub-dataset with equal number of sample coded as 0 and 1\n",
    "\n",
    "class_counts = y.value_counts()\n",
    "minority_class = class_counts.idxmin()\n",
    "majority_class = class_counts.idxmax()\n",
    "n_minority = class_counts.min()\n",
    "\n",
    "# Indices for each class\n",
    "minority_idx = y[y == minority_class].index\n",
    "majority_idx_sampled = y[y == majority_class].sample(\n",
    "    n=n_minority,\n",
    "    random_state=12434  # reproducibility\n",
    ").index\n",
    "\n",
    "balanced_idx = minority_idx.union(majority_idx_sampled)\n",
    "\n",
    "X_bal = X.loc[balanced_idx]\n",
    "y_bal = y.loc[balanced_idx]\n",
    "\n",
    "# Shuffle rows\n",
    "X_bal, y_bal = shuffle(X_bal, y_bal, random_state=14312)\n",
    "\n",
    "print(\"\\nBalanced class counts:\")\n",
    "print(y_bal.value_counts())\n",
    "\n",
    "#Using the balanced dataset to train the machine learning logistic regression model\n",
    "\n",
    "bal_classifier = LogisticRegression(max_iter=1000)\n",
    "bal_classifier.fit(X_bal, y_bal)\n",
    "\n",
    "print(\"\\nClasses in classifier:\", bal_classifier.classes_)\n",
    "\n",
    "# Predict on balanced data (for quick check)\n",
    "predicted_bal = bal_classifier.predict(X_bal)\n",
    "\n",
    "# Print out the evaluation metrics\n",
    "\n",
    "print(\"\\nLogistic Regression Accuracy (Balanced Data):\",\n",
    "      metrics.accuracy_score(y_bal, predicted_bal))\n",
    "print(\"Logistic Regression Precision (Balanced Data):\",\n",
    "      metrics.precision_score(y_bal, predicted_bal))\n",
    "print(\"Logistic Regression Recall (Balanced Data):\",\n",
    "      metrics.recall_score(y_bal, predicted_bal))\n",
    "\n",
    "print(\"\\nClassification Report (Balanced Data):\\n\",\n",
    "      metrics.classification_report(y_bal, predicted_bal))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_bal = metrics.confusion_matrix(y_bal, predicted_bal)\n",
    "\n",
    "sns.heatmap(cm_bal, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix (Balanced Data)')\n",
    "plt.xlabel('True Willpower')\n",
    "plt.ylabel('Predicted Willpower')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3ee422f0da69d",
   "metadata": {},
   "source": [
    "To interpret the linguistic patterns associated with human codings, I extracted the model coefficients corresponding to each TF–IDF feature. Positive coefficients indicate words that increase the likelihood of the response being classified into the positive class (coded as 1), whereas negative coefficients indicate words predictive of the negative class (coded as 0). I ranked the coefficients and reported the top 20 positively weighted features and the top 20 negatively weighted features to highlight the most influential lexical predictors identified by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16841ee54a71c71d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:00:23.069913Z",
     "start_time": "2025-12-01T02:00:23.031196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 positive and negative predictors for Willpower:\n",
      "   feature_pos  coefficient_pos  feature_neg  coefficient_neg\n",
      "0        focus         1.024687          use        -0.439765\n",
      "1    improving         0.989208       myslef        -0.441163\n",
      "2         dont         0.911352         calm        -0.450773\n",
      "3       trying         0.691945        eagle        -0.466036\n",
      "4         said         0.676812      academy        -0.466101\n",
      "5         done         0.669538        solve        -0.469132\n",
      "6    realizing         0.640896    finishing        -0.480019\n",
      "7     question         0.622145         help        -0.484462\n",
      "8        would         0.606577       harder        -0.485194\n",
      "9           ok         0.578578      keeping        -0.496969\n",
      "10       stuck         0.554590    something        -0.504390\n",
      "11     teacher         0.550984   motivation        -0.513434\n",
      "12     thought         0.542113          idk        -0.515229\n",
      "13       never         0.539773      reading        -0.527796\n",
      "14      worked         0.516641  motivatated        -0.527907\n",
      "15       super         0.500584      problem        -0.537149\n",
      "16        told         0.484502         even        -0.537620\n",
      "17        need         0.484151         work        -0.577516\n",
      "18      people         0.474104         time        -0.583980\n",
      "19          se         0.469404       wanted        -0.909499\n"
     ]
    }
   ],
   "source": [
    "# Print out the logistic coefficients for the top word list\n",
    "# For binary classification in sklearn:\n",
    "# bal_classifier.coef_[0] corresponds to the negative class (0) in bal_classifier.classes_\n",
    "positive_class = bal_classifier.classes_[1]\n",
    "\n",
    "coef = bal_classifier.coef_[0]         # shape: (n_features,)\n",
    "feature_names = X_bal.columns          # TF-IDF feature names\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coef\n",
    "})\n",
    "# Sort descending: largest positive coefficients first\n",
    "coef_df_sorted = coef_df.sort_values(by='coefficient', ascending=False)\n",
    "# Top words predicting the positive_class and negative_class (Top 20 for each)\n",
    "top_20_pos = coef_df_sorted.head(20)\n",
    "top_20_neg = coef_df_sorted.tail(20)\n",
    "\n",
    "highest_20 = top_20_pos.reset_index(drop=True)\n",
    "lowest_20 = top_20_neg.reset_index(drop=True)\n",
    "\n",
    "highest_20.columns = [col + \"_pos\" for col in highest_20.columns]\n",
    "lowest_20.columns = [col + \"_neg\" for col in lowest_20.columns]\n",
    "\n",
    "coef_compare = pd.concat([highest_20, lowest_20], axis=1)\n",
    "\n",
    "print(\"Top 20 positive and negative predictors for Willpower:\")\n",
    "print(coef_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7995594e637d5",
   "metadata": {},
   "source": [
    "For Performance Self-talk and Effort, the results of logistic regression and the top 20 positive and negative word-predictors are reported below.\n",
    "\n",
    "<img src=\"TF-IDF Assignment_files/images/Full_Logistic_Performance.png\" alt=\"Results for Performance Self-talk\" width=\"400\">\n",
    "<img src=\"TF-IDF Assignment_files/images/top20_Performance.png\" alt=\"Results for Performance Self-talk\" width=\"600\">\n",
    "<img src=\"TF-IDF Assignment_files/images/Full_Logistic_Effort.png\" alt=\"Results for Effort\" width=\"400\">\n",
    "<img src=\"TF-IDF Assignment_files/images/top20_Effort.png\" alt=\"Results for Effort\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9bf06c11083d9",
   "metadata": {},
   "source": [
    "To complement the full TF–IDF classification model and to enhance the interpretability of the lexical patterns associated with human-coded motivational strategies, a reduced-feature analysis was conducted. Although the full TF–IDF matrix provides a comprehensive representation of the lexical content in students’ responses, its high dimensionality includes many features that are statistically weighted but conceptually uninformative. In contrast, limiting the model to the strongest positive and negative predictors enables a deeper analysis of the lexical cues most associated with the human-coded motivational strategy.\n",
    "\n",
    "Following the initial logistic regression model, the complete list of TF–IDF coefficients was examined to identify the most influential positive and negative predictors of the target motivational strategy (coded as 1 vs. 0). This manual review was necessary because the full TF–IDF representation includes many statistically weighted but conceptually uninformative features, such as rare tokens, idiosyncratic word forms, or terms lacking theoretical relevance. A reduced TF–IDF matrix was constructed using only these selected predictors. A logistic regression classifier was then trained on the same balanced dataset using this reduced feature space. Model performance was evaluated using accuracy, precision, recall, and a confusion matrix summarizing prediction errors. Because the model was evaluated on the same balanced dataset on which it was trained, these metrics reflect the classifier’s ability to distinguish between the two human-coded categories based solely on the most influential TF–IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eb34e5a99eff404",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:14:17.939404Z",
     "start_time": "2025-12-01T02:14:17.833884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First top 50 positive words for Willpower:\n",
      "['focus', 'improving', 'dont', 'trying', 'said', 'done', 'realizing', 'question', 'would', 'ok', 'stuck', 'teacher', 'thought', 'never', 'worked', 'super', 'told', 'need', 'people', 'se', 'everything', 'cant', 'sooner', 'smarter', 'candy', 'thanks', 'ertyuio', 'solved', 'guessing', 'bobathan', 'idl', 'tried', 'ask', 'give', 'belving', 'jusr', 'learnes', 'knowledge', 'twostep', 'w', 'cooked', 'felt', 'always', 'torcher', 'motivated', 'keept', 'okay', 'bit', 'cause', 'lisin']\n",
      "\n",
      "First top 50 negative words for Willpower:\n",
      "['hard', 'best', 'instead', 'used', 'math', 'self', 'mativate', 'someing', 'grinding', 'way', 'go', 'zero', 'nkt', 'better', 'giveing', 'im', 'say', 'th', 'highschool', 'leanr', 'scholerships', 'think', 'motivates', 'skill', 'someone', 'theyre', 'typically', 'waht', 'highlight', 'consider', 'streghths', 'well', 'much', 'number', 'maybe', 'tofay', 'worth', 'hecking', 'hit', 'get', 'l', 'understanding', 'attempting', 'ill', 'tring', 'prov', 'ided', 'one', 'already', 'away']\n"
     ]
    }
   ],
   "source": [
    "# Print out more positive and negative predictor words\n",
    "pos_df = coef_df_sorted[coef_df_sorted['coefficient'] > 0].copy()\n",
    "neg_df = coef_df_sorted[coef_df_sorted['coefficient'] < 0].copy()\n",
    "\n",
    "pos_words = pos_df['feature'].tolist()\n",
    "neg_words = neg_df['feature'].tolist()\n",
    "\n",
    "print(\"\\nFirst top 50 positive words for Willpower:\")\n",
    "print(pos_words[:50])\n",
    "\n",
    "print(\"\\nFirst top 50 negative words for Willpower:\")\n",
    "print(neg_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb90d97f3a86cb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T02:15:29.248996Z",
     "start_time": "2025-12-01T02:15:29.044076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final model for predicting Willpower using top 50 positive + top 50 negative words ===\n",
      "Total usable features: 100\n",
      "Accuracy: 0.7692307692307693\n",
      "Precision: 0.8414634146341463\n",
      "Recall: 0.6634615384615384\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.88      0.79       104\n",
      "           1       0.84      0.66      0.74       104\n",
      "\n",
      "    accuracy                           0.77       208\n",
      "   macro avg       0.78      0.77      0.77       208\n",
      "weighted avg       0.78      0.77      0.77       208\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAE6CAYAAADQlHMqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyQUlEQVR4nO3dCXhM5/4H8N8JshBriNpLlEttSSzVW62itVetRWurEq2ldauU2FXtiga11HKrtdPqv65WUUXVkggiaGJXoomKNRJL/s/3vc/MnZkkMkMmMyfn+/GcJ+bMZObMJPme9/zOe95XS01NTRUiInJrHq7eACIiyhzDmohIBxjWREQ6wLAmItIBhjURkQ4wrImIdIBhTUSkAwxrIiIdYFiT4fA6MNIjhrUTHT16VD766CNp2LCh1KhRQ5o0aSKjRo2SCxcuOO01ly1bJv/85z/V682bNy9LnnPfvn1SuXJl9dXZTK+FZffu3ek+5tSpU+bHXLx40e7nTklJkU8//VS+//77TB+L5/7888/lSd27d0/atWsnv/32m7rdrVs3tWSXIUOGmD8ry2XLli3mx9y+fVvGjRunfm8CAwOlT58+cvr0afP9165dU7/Dzvy9pczltuMx9Bi+/vprFQz16tWTDz/8UPz9/eXcuXPy5Zdfyk8//STLly+Xf/zjH1n6mrdu3ZIpU6aoP6y3335bSpcunSXP++yzz8rq1aulYsWKkl08PDxUoLzwwgtp7tu8efNjPedff/2lPvdJkyZl+li836eeekqe1BdffKGe5/nnnxdXOHHihLRq1SrNDuLpp582/x+/n4cPH1YNC19fXwkLC5Pu3bvLDz/8IAULFpTChQtLz549ZcSIEfLvf/9bNE1zwTshHBJSFjt48GBqlSpVUj/55JM09129ejW1QYMGqW3bts3y17148WJqpUqVUtetW5eqV7///rt6D127dk2tW7du6r1799I8plmzZqlt2rRRj7tw4YLdz43H4nvWr1+fmh2uXLmSWq1atdSjR4+a17311ltqyQ53795NrVq1auqaNWsyfExERIT6TH755Rer39FatWqlzps3z7wuOTlZ/Tx+/PFHp283pY9lECdA6zl//vzyr3/9K819RYoUkY8//lgaN24sd+7cUesePHigWuKtW7dW5Qu0jKdPny7Jycnm78P3oHWzfv16adq0qVSrVk3atGkjv/76q7p/w4YN0qhRI/V/tIBwqAtYh++1hMdalhDu3r0rY8eOlRdffFE9b7NmzdR7eFQZBCWe3r17qyOHoKAg6devn8TExKT5nr1796pWfs2aNdVh9rRp09T7zUyLFi0kMTFRfv/99zQtxbNnz0rz5s3TfM/PP/8sXbt2VYfypveBzxXwXvGZw/Dhw82fFT6bHj16yJgxY9T7wOti+yzLIAMGDJDq1atblQZwX5UqVWT//v0ZvoelS5dKyZIl1bZkZNeuXer+0NDQDGvp6ZUxTIvpfaTnjz/+kPv376vtzAhKTXnz5rU6gsHvaJ06dWTnzp3mdZ6enur3bsGCBRk+FzkXyyBZDH9w+APAH5GPj0+6j0EgWBo9erR89913qlZYu3ZtiY6Olrlz58rx48dl8eLF5sPOqKgodSg/aNAgdbg6e/ZsGThwoApsBDwOXxEs7777rrptL5RrsM3Dhg2TokWLquebOnWqFCpUSNq3b5/m8QjQd955RwU1vhc7FfwRd+7cWdasWSMBAQFWNVMEKN7bL7/8ot5PmTJl1GMfBSWXZ555Jk0pBIfmdevWlWLFilk9Hs/dv39/dfiOzwQ7oG+++UbGjx+vwhCBZfn5vPrqq+bvPXjwoHh5eanPHDvQXLlyWT03dmQtW7ZUgf7VV1+pnwPKG9gJYVsygto46tUZOXDggNoe7KQ/+eSTDMsLKMlkBCGaEezYYO3atWpnip0fGgP4OWPnaar/o1xm+57Lli2bpraPnR+25cyZM1K+fPkMX5ecg2GdxXAyBuFlb704NjZW1q1bp+qGffv2VevQAkWNe+jQoSo4X3rpJbX+5s2bqlWMPyRAi+itt95S4YlWj6kFhftr1apl9zajdYjXRCABQhjP7efnl+7jZ8yYIeXKlZOFCxea/8gRqK+88orMmTNH7URMOnbsqEIU6tevr1q/CNbMwhrQekaNFGGZO3duc70awZPe59i2bVvVQjVBCxvvBa18hJPl51O1alXz49D6RKhnVKPGDgxBPXjwYBV8qHtXqlRJ3n///Qy3HSEYHx+vwjE9R44ckZCQEPVzmzhx4iPrwI78LC1hZw9JSUnqZ4awxs8MOzSELs6Z4HcKO35b+fLlUyceLeHoAnC0xLDOfgzrLGYKL3sO9cF0GG0KShPcxuE6gsYU1jg8NQU1mMIFf4xPAoG2atUqiYuLU6+FxRSwttDyRAkELULL1liBAgXk5Zdftjp0NgWmJWyzqfyTGRyBIPyxM8LOACfBrly5olrF27Zts3osWvqAgEHL7/z582o7Tb1AHgVHEJmdTMS2oJWPoyC0ZrHTfFSr1tRzIr2d9qVLl9SRBo7C8Hw4mfoo2JlkBCFv2yo2wY4cP5MGDRqY12GHic8PRwazZs16ZDdG2x0ISnv4OTvSA4eyDmvWWQxnz9EqwR9kRhBW169fV/83fbU9rEdLEmfh0fIxsS2rmP6YHj58+ETbjNboBx98oP4IJ0yYoLoYouVrOoy2hO3BHzham7awznJ7wdvb2+o2gsnefs5ovaE1bOpmhlY1Qhufsa2///5blT9QRurUqZOqKaN3DGT2evh52QMtd3zW6EmRWcvS9DmkVwrD54zSDLr1oTRjT2+cjBYczWSkQoUKVkENCFvU5k0/W7SqbVvQgHUIZ1t4P6bPlbIXW9ZOgEBBixjlENRCbaGuiy52KH+YggeHzKVKlTI/Bn/IKKkgsJ+UbSvftmWLFiLquFiwk9mxY4fqo43SDGrElvAHjJ1EQkJCmtfBe0ArNSuhRYuTnShDILRRA08P1uMEIPqZozWP94QjDnzWWQHPhS5/KH/gxN2SJUvMrfn0mH5uN27cSHMfngM1frRs8Tw4ijKVGNKD35OMPKp1j50bwtm2+yN+L3GUBtjp4HwFdkKWLXx0M7U892CC95MVv5PkOLasnQAnnlAfxB9jeoGGP1CcQEPLyHSCyjYUcRshGxwc/ETbgpYTyhuWwsPDzf/HiTjUTbFNgN4Lb775pgqQ9I4OUMtGq/A///mP1U4ALUnUop90e9OrW+OzxGE7jkJMPTps4T3h8B4lHVOAmXrKmI48MioX2AM1X3yOaLGjvIDyDOrSGcHnCLafPWCHhiMnlJpQfhk5cuQjSx0I8owWU6+f9KC0hZ2cZRkIZaSIiAj1OQGCHK1o9EqxPErBSVecx7CEzx87LdN7o+zFlrUT4IQQTj4hrPEH/frrr6vWCLq2oZWIlo0pyBHaOLzGHz/+ENBlCieGcHiMPyjbw1hHoWaJVhwWnGTbvn27VXc4lCmw08Dr5cmTR/3xo+a7ceNGFeLpQYsb3fZwQhQ9PXAUgBNXCIWMat2PCz1HEErYfhzyY2eRHpzIQ+8FvBcEIAIJ24SjAFNN33RYjxNkaDWaekTYc15hxYoV6gQjSiAoGW3dulV1+0MgprcTQAkCoYadSEalCpQUEKb4HPF7gROOWem9996TXr16qa84qYiwxc8ZOws0KAC/b2gw4IIYLLgPOyR8Vl26dEl3J5/ehUrkfGxZOwlKCggLQPc2/EHiDx5d6r799lurQ0z0BkDIIWzwOPQNxh/XokWLMj35lBkEAHpkIAywTWjZ4/UsoScEupihdY0/YpRAOnTooHphpAcnqdCHGK1y9CXHJfTFixdXJQcc4mc1lEKwQ7A9CWtp8uTJKnxRc8dniROQuIQawYJWoukoA+GFHik4wYfnzAxKRjjRi/eFHZSpxo0Tg+jRga6IGcHOztS6zwhO5uJx6DaI/uNZ6bnnnlM/U7wH7Gjwc8bODL9flvVoBDiOWNBdEzsg/CxRTrI9N4D3gp2iZbmOso+GK2Oy8fWIDAMlB5ysRWCiBatnCHwc5eFcC94TZT+2rImcBC1UXHWKIyS9Q7kHFylldM6AnI9hTeRE6E6IFnZGIwjqAU44oiyCVjUHcXIdlkGIiHSALWsiIh1gWBMR6QDDmogoi1y9elWNiolhD9C/HmPIWI4XgxPOuA4D3VEdPY/Bi2KIiLIATv+hjz+umMVokTixjOFo0b8fwY370F8fY9Kjrz8GQ8OQAPZeEZojw9oncICrN4Gy0bUDmQ+GRDmHd+7sy4ekQ/b/bmGc80OHDqkgxpW3GIYX48eYJiNByxpdIHEVLi6Kw5W0CG70GLIHyyBEZCyah/2LAxDGGCALQW2C4RsQ4rhUH+FtOVwCxtGJjIy0+/lzZMuaiChDHvYP6IXxbmzHQ8dAYemNdmgaIhhj0ZiGxsVAXhikC8M8YEIRS5jcI72BvjLcbLsfSUSUE2ia3QsGEEML2HLJaB5KjE2DQMb4NLg8H8PMYgwdQODbBjxuZzYxhiW2rInIWDQPhwZCw+Bf9owhjrHrMZomRmVEqKPljJo1xkHHlZ+2wYzbtpNzPArDmoiMRbP/kvmMSh4ZwaiEGIYYZQ8Mi7xnzx71FdPx4f+WMIGHbWnkUVgGISJj0ZxzghGTZGAMcMzwhGn6MMEEJuTAeOEokRw7dkwNK2yCk472jqkODGsiMhbN/pq1IzBxA2rV06ZNUz1D1q5dq7rmoRSCwC5RooQaGx2TkGCse4yHjnHj7cWwJiJj0ZzTsobPPvtMBXXr1q1l+fLlMnv2bFUawWxCmNQD5RFM9LFp0yY14YQjU6TlyFH3eFGMsfCiGGPxftKLYv4Zavdjk/ZYz6rkSjzBSETGoumzoMCwJiJj8Xj8We5diWFNRMaisWVNROT+PPQ5NRnDmoiMRWPLmojI/WlsWRMRuT+NLWsiIvensWVNROT+NLasiYjcn8aWNRGR+/PgRTFERO5PYxmEiMj9aSyDEBG5P40tayIi96cxrImI3J/GMggRkfvT9Nmy1udWExG52RyMcPnyZQkJCZGgoCBp1KiRLFu2zHxfdHS0dOzYUU2S2759e4mKinLouRnWRGQsmvPmYPzggw8kb968smHDBhkxYoTMmjVLtm7dqibS7du3r9SuXVvdFxgYqEId6+3FsCYiQ9E8POxeHHH9+nWJjIyUd999V55++mlp0qSJNGjQQPbu3SubN28WLy8vGTp0qAQEBEhoaKjky5dPtmzZYvfzM6yJyFA0TbN7cYS3t7f4+PiolvO9e/fk9OnTEhERIVWqVJHDhw9LcHCw+TnxFaUShLu9GNZEZCya/UtKSorcunXLasG69KDlPHr0aFm9erWqSzdv3lxefPFFVaeOj48Xf39/q8f7+flJXFyc3ZvN3iBEZCiaAy3mBQsWSFhYmNW6AQMGyMCBA9N9/KlTp+Tll1+WXr16SUxMjEyYMEHq168vSUlJ4unpafVY3M4o+NPDsCYiQ9EcCGucBETwWrINXRPUptetWyc7d+5UJZHq1avLlStXZP78+VKmTJk0wYzbeJy9WAYhIkPRHKhZI5h9fX2tlozCGl3xypUrZxXAVatWlUuXLknx4sUlISHB6vG4bVsaeRSGNREZiuakE4wI3nPnzlm1oHGSsXTp0qqGfejQIUlNTVXr8RUnH7HeXgxrIjIWzYHFAbgIJk+ePDJy5Eg5c+aMbN++Xb744gvp1q2bNGvWTG7cuCETJ06U2NhY9RV1bJyEtBfDmogMRXNSyzp//vzqikX0/OjQoYNMmjRJ9bl+4403VPkEJyvDw8OlXbt2qivfwoUL1QU0dm93qqldnoP4BA5w9SZQNrp2wPpsPeVs3k/YLaLwW1/b/dhrK94Ud8HeIERkKB4OXpnoLhjWRGQsmugSw5qIDEXjeNZERO5PY1gTEbk/jWFNRKQDmugSw5qIDEVjy5qIyP1pDGsiIvenMayJiNyf5sGwJiJyexpb1kRE7k/TaVjr8yJ5gytW2Fe+mdZbLv86VaK+GyNvta6X5jEVyhSVv/fOdMn2kXNgnOR2bVrJgf37zOv27N4lHdu+JnWDaqivu3ftdOk2GnnUPWdjy1qHVs/sI7k8PKRZnzlS0r+QLJ7QTW7evivfbT+s7i9dvJBsmN1PfLzTn9GC9Cc5OVk+HvqhnIqNMa87f+6c/Ov9ATJg0GB5uVFj2b7tZ/lgYH/57octUqpUaZdur1vTRJfYstaZoKplpX6tAOkxYpkcPnlR/rMrSmYu2yqDezRR97duWEP2fDNMku/dd/WmUhY5FRsr3bp0kovnz1utv3IlTtp36CTdevSU0mXKSPeevcTHJ69EHT3ism3VA02nLWuGtc6UL+Unf/19U87+edW87mjMJQmqUlZy5/aQZg2elfHz/k+GTF3v0u2krBN+cL/UqVtP/v3Naqv1WDd0eKj6/71792TD+rWSci9FqlWv4aIt1QdNp2HtFmWQa9euqXqcj4+PFChQwNWb49au/H1TCuX3ER/vPJJ0955aV7p4YcmTJ5cU9PWR/hNWqnUNgp9x8ZZSVunUuesj70c55PXWzeXBgwfy/uAPWQLJhLuFsNuH9U8//SQrVqyQI0eOqHqcCWYGrlatmvTo0UOaNPnvoT39z4GjZ+Vy/HWZOayjfDhlnTxVrIAMeutldZ9nHrfY91I2K1ykiHy9ep0ciTwk06dOlrJly0mTV5u6erPclsawtt/SpUslLCxM3nnnHRkwYID4+fmp6d3Rusb07AcPHpSPP/5Y3n//fTXZJP1Pcsp9efOjL2XF1Lflr93TVUnks+U/y9Qh7eXGrSRXbx65AOb+q1KlqlpOnTolK79ZwbB2wUUxGzZskOHDh6d9PU2TEydOSHR0tIwZM0b++OMPqVixoowbN041TN06rJcsWSJTpkxJt+UcEBAg9erVk8qVK8uECRMY1ukIjz4vVVqNleJ++SUh8bY0qf8Pib92U24npbh60ygbxcbGyI3r1yUouLbV38/BA/tdul1GbVm3aNFCGjRoYL59//59VSFo2LCh3LlzR/r27SutW7eWyZMny8qVKyUkJES2bt1q96S5LjnBePfuXSld+tF1teLFi8vNmzezbZv0onCBvLJtyWApUjCfXLl6Ux48eCjNXqgmuw7+r0sXGcPOHTtk3JiRYjnndXT0MalQoYJLt8vdaZr9iyNQwi1WrJh52bRpk/rZDBkyRDZv3ixeXl4ydOhQtUMNDQ2VfPnyyZYtW+x+fpeE9SuvvKLKHCh3YO9j6eHDhxIRESEjRoyQpk15KGfr2o07ki+vl0z8oI08XcpPeratLz3aPCczl/3s6k2jbNaq9WuSEB8vs2ZOl3Pnzsqqb76WH77fJL37hLh608TovUESExNl0aJF8uGHH6oS7+HDhyU4ONj8nPgaFBQkkZGR7l0GGTt2rCqD9O7dW53BLlSokLlmjTeZO3duadOmTbr1HxLpNmyJhI3sIgfXjlBd+N4cukSVRshYij/1lMxf+KVMnfyprPpmhZQsWUqmz5wtVao+6+pNc2uaAxmMTMJiCVmF5VFQ5vD395dmzZqp2/Hx8apObQnn6mJiYtw7rPFGR40apQ4PUHjHG0lKSlKHCSh/VKlSRR1SUPpizv0lTfvMfuRjdoXHiE/ggGzbJsoeh4+dtLpdo2YtWbFyjcu2R480B9J6wYIFqjOEJXSKGDhwYIbfg9LH2rVrVQcKE+SbbcCbGqj2cmlfL/SrDgwMdOUmEJHBaA60rHESsFevXlbrMmtVHz16VK5cuSItW7Y0r0ND1DaYcduRRik75hKRoXg40HXPnpKHrV27dknt2rWlYMGC5nWoGKBbsiXcRqnEXrzcnIgMRXNSbxATXOiHk4eWatasKYcOHTL33MFXdKTAensxrInIcC1rDzuXx4GThrYnE3Gi8caNGzJx4kSJjY1VX1HHbt68uf3b/VhbQ0SkU5qTu+6hvGE7xpGvr686WRkeHi7t2rVTXfkWLlxo9wUxwJo1ERmK5uSxQVAGSU+NGjVk48aNj/28DGsiMhRNn+M4MayJyFg0naY1w5qIDEXTZ1YzrInIWDSdpjXDmogMRdNnVjOsichYNJ2mNcOaiAxF02dWM6yJyFg8nDStl7MxrInIUDSdNq0Z1kRkKJo+s5phTUTGouk0rRnWRGQomj6zmmFNRMai6TStGdZEZCgaw5qIyP1p+sxqhjURGYum07RmWBORoXjwohgiIven6TOrOQcjERmLh6bZvTgqJSVFxo0bJ3Xq1JHnn39eZs6caZ7RPDo6Wjp27KhmNG/fvr1ERUU5tt0Obw0RkY5pmv2Loz755BP57bff5Msvv5QZM2bImjVrZPXq1XLnzh3p27ev1K5dWzZs2CCBgYESEhKi1tuLZRAiMhTNSXWQxMREWb9+vSxdulRNjgtvv/22msk8d+7c4uXlJUOHDlWvHxoaKr/++qts2bJFzXZuD7asichQPDT7F0eEh4eLr6+v1K1b17wOrelJkyapwA4ODjbvKPA1KChIIiMj7d9uxzaHiEjfNE2ze0EN+tatW1YL1qXnwoULUqpUKfn222+lWbNm0rhxY5k7d648fPhQ4uPjxd/f3+rxfn5+EhcXZ/d2swxCRIaiOdBiXrBggYSFhVmtGzBggAwcODDNY1F/PnfunKxatUq1phHQo0ePFh8fH0lKShJPT0+rx+N2RsGfHoY1ERmKJvanNU4C9urVy2qdbeiaoC6NljdOLKKFDZcuXZKVK1dKuXLl0gQzbnt7e9u9LQxrIjKUXA4UoxHMGYWzrWLFiqmTiKaghvLly8vly5dVHTshIcHq8bhtWxp5FNasichQNCd13UP/6eTkZDlz5ox53enTp1V4475Dhw6Z+1zja0REhFpvL4Y1ERmKh5MuiqlQoYI0bNhQhg8fLidOnJBdu3bJwoULpUuXLuqE440bN2TixIkSGxurvqKO3bx5c/u3+zHeKxGRbmlOvChm+vTpUrZsWRXQw4YNkzfffFO6deumuvThZCW696FfNbryIcjz5s1r93OzZk1EhqI5cXCQ/Pnzy9SpU9O9DxfKbNy48bGfm2FNRIai6XQgJ4Y1ERmKh07TmmFNRIaiiT4xrInIUDS2rImIctZFMe7ksbruPXjwQH755RdZtmyZ6juIbig3b97M+q0jItJR1z23alnj0snevXursVuvX7+uRpZavHixujoHA25XrlzZOVtKRGTgMojDLevx48ercVlxdY7pmnlMXYMpbDBLAhGREcezdruwPnjwoJr9IFeuXOZ1efLkkffee8/hOcWIiNx5PGtdhzWG9Lt69Wqa9Ri8BJdUEhG5M82BRddh3blzZzWgNk4wmkIa846NGjVKOnTo4IxtJCLSxezmbnWCsX///lKgQAEZO3asGjUKc4xhepqePXuqE49ERO5Mc68Mdm4/a4wihQXT2KAbHwYvISLSA02nae1wWGMyyEd5/fXXn2R7iIicStNnVjse1nPmzLG6jZY1Tjhi/jEMAciwJiJ3lsvd+uQ5K6y3b9+eZt3t27fVSUdeEENE7k7TadNaSzVNCvaEzp49q2ZH2Lt3r7jathPWE1NSzjboqwhXbwJlo2MTX32i7x+48bjdj/28bRVxF1k2rRfmHHv48GFWPR0Rke4uitm6dauqMFgugwYNUvdFR0dLx44d1SS57du3d/giQofLIOgFYvsmUAY5efKk6r5HROTOPJxYBcFkuC+//LJMmDDBvM7Ly0v1nEM359atW8vkyZNl5cqVEhISosLd3nkYHQ7revXqpVmHMUKGDBki9evXd/TpiIhyTFifOnVKKlWqJMWKFbNav27dOhXaQ4cOVY3d0NBQ+fXXX2XLli1qAl2nhDVG2+vevbuawZeISG80J55gRFhjUDtbGEYaA+CZXhtfg4KCJDIy0u6wdrhmvWnTJvHwyLJSNxGR2466l5KSIrdu3bJasC496KuB4Td2794tTZs2lSZNmsj06dPV4+Pj48Xf39/q8bjyOy4uzu7tdrhljbr0uHHj1NeSJUuqpr0lrCMicleaAw3rBQsWSFhYmNW6AQMGyMCBA9M89tKlS2oIDpSFZ82aJRcvXlTDRt+9e9e83hJuZxT8jx3WBw4ckMDAQHXhi+miGIxnDaZmPfYq+P/x4/Z3iyEiym65HUhrnATs1auX1Trb0DUpVaqU7Nu3TwoWLKiysEqVKqqH3EcffSR169ZNE8y4jVFM7d5uex6EGjWa9mi2b9u2ze4nJyLSc8va09Mzw3BOT6FChaxuBwQESHJysjrhmJBgff0HbtuWRh7FruKz5XUz2Hs8aiEiMuIQqbt27VK95VDyMEGlAQGOk4uY+tCUpfgaERGh+lzbvd05/RJNIqLsmDAXpWKcwxs5cqScPn1adu7cKVOnTpV33nlHmjVrpiYXnzhxouqLja8I9ebNm2f9CUZccWNPLxCWSYjIiP2sfX191aThn376qcrLfPnyqclaENZo7OJk5ZgxY2TNmjXqysaFCxfafUGMQ2GNIjvHrSYivfNwYpXgmWeekaVLl6Z7H0Yl3bhx42M/t11hjb1Cy5Yt1QlGIiI903Ra0bUrrLNoYD4iIpfzyMlh3bZt2zQXvxAR6ZHmdvOWZ2FYT5o0yflbQkSUDXJ7GGjCXCIivdJ0WrRmWBORoXjoM6sZ1kRkLBrDmojI2P2snYlhTUSG4qHPrGZYE5GxaAxrIiL355GT+1kTEeUUmj6zmmFNRMaSW6dFa4Y1ERmKps+sZlgTkbF46DStGdZEZCiaPrOaYU1ExuIh+sSwJiJD0XTatNbrToaI6LFoDiyPq2/fvvLxxx+bb0dHR0vHjh3VbOaYnzEqKsrh52RYE5HhTjB62Lk8jh9++EHNbG5y584dFd61a9eWDRs2qFnQQ0JC1HqHtvuxtoaISKc0J7asExMTZerUqVK9enXzus2bN6uZtoYOHSoBAQESGhqqZj7fsmWLQ8/NsCYiQ9E0+xdHTZkyRdq0aSMVK1Y0rzt8+LAEBweba+X4GhQUJJGRkQ49N8OaiAwll6bZvaSkpMitW7esFqxLz969e+XgwYPy3nvvWa2Pj48Xf39/q3V+fn4SFxfn0HYzrInIUDRNs3tZsGCBahVbLlhnKzk5WcaMGSOjR48Wb29vq/uSkpLE09PTah1uZxT6GWHXPSIyFM2Bx+JEYK9evazW2QYvhIWFSbVq1aRBgwZp7kO92jaYcds21DPDsCYiQ9EcKEYjmNML5/R6gCQkJKieHmAK5x9//FFatWql7rOE27alkcwwrInIUDyc8JxfffWV3L9/33x7+vTp6uuQIUPkwIEDsmjRIklNTVU7CnyNiIiQfv36OfQaDGsiMhTNCVcwlipVyuo2uuZBuXLl1MnEGTNmyMSJE6Vz586yatUqVcdu3ry5Q6/BE4xEZChaNlzBaMnX11edlAwPD5d27dqprnwLFy6UvHnzOvQ8bFkTkaFo2TA0yOTJk61u16hRQzZu3PhEz8mwJiJD8eAcjERE7s9Dp6PuMayJyFA0fWY1w5qIjMWDZRAiIven6TOrGdZEZCwaw5qIyP1pLIMQEbk/D31mNcOaiIxFY8uasstfly/K6i9myOkTRyWvbwFp2LK9vNLuTXXfmkWz5Jf/W2v1+E59B0vDlh1ctLX0pPLk0mRYi8rSomYJuffgoWw4+KfM3hqr7nu+op8MaVZJyhTxkcMXrssn3x+XswmOze1nNJo+s5phrTcPHz6UeeOHSLlnqsjwz5bKX5cuyJIZY6WQXzGp89KrEnfhjLTp1k+ea9zC/D0+ef87qAzp0/BW/5B6FYpIyLJwyeuZW6Z3riGXEu9KxLlrMq97oCzeeUb+7/BlaR9cSpb0ri2tPtsjd1IeuHqz3VYunaY1B3LSmZuJf0vp8s9Il35DxL9kGalW+3mpXCNYYo8fUffHXTwnZQIqScHCfubF08uxQc7JfRT0yS3tgkvJmI3RcvTiDdl3+m9Zvvus1ChTUDrXKyOR5xMlbNsp1Zqe8WOM3Lp7X1rWLOHqzXb7Mohm5z93wrDWmYJFiso7QyeId958alzcU8ePSOyxw1KpWqAk3bktiVfjpXjJsq7eTMoiQeUKqwA+ePaaed3iX8/KqA3HpHRhHzl64brV4/+IuyW1yhZ0wZbqh+bECXOdiWUQHRvVp738HX9FqtX5pwTWbyjnYk+osXq3rF0uxyJ+l3z5C0rjNm/Ic43+VxIhfSldxEf+TEyS12qVkD4NK6j69bcRl2TBL6fl6q0U8S9gfdT0VEFvuZ50z2Xbqwea6BPDWsf6DJsoNxL/lpXzp8u6L+dI2YqVVXOgeOly8lKrDhITdUi+mTtVvH3ySa36L7l6c+kx5PXMJeX88kqnuqVl5PooKZbfS8a8XlWSUh7IlqNxEvZWoGw+cll2x1yVVjVLSLXSBWT/6b9dvdlujQM5UbbDSUa4l5Iiy2aOkxm9fpLqdV6QfPkLqPWln66oTkD+umUjw1qnHjxMlfzeeeSjNUflcuJdta5EIW9Vr2752R6Zt+OUzOpaS3J5aCqkNx26pB5PGdNnVLswrDEvmb3q1Knj1G3RE7SkT5+IklrPvWheV6LM03L//j1JTrojvgUKWT3+qdJPyx9Hwl2wpZQV4m+myN17D8xBDWcS7qhyByz85Yws3XVWBfTft1NkRuca8ue1JBdusQ5ooksuC+vx48dLbOx/+4riRFlGUIM9fvx4Nm6Ze7t65ZIsmjxCJn65UXXXg/OnTopvwUKy4/u1KsjfnzDb/PiLZ2JUWYT06fCFRPHO899SyLmr/+0/HVAsnwrkFjWekhqlC8rkzSdVUHvl9pC6FYpI6PooV2+2W9N0mtYu6w2yfv16ady4sVSuXFnNSXbixIl0Fwa1tXIVq0iZgMry1ZxP5fL5MxJ18DfZuGyuNOvYQ6rXfUFijh2SrRu/kfjLF+XX/2yUfTu2SJPXu7p6s+kxoUveLyfiZWL7alL5KV/5Z0U/6f1ieVm9/6KcTbitatlNqvpLWb+8MvWN6hJ3/a7s+iPB1Ztt2N4g586dk969e0tgYKA0bNhQFi9ebL7vwoUL0rNnT6lVq5a0aNFCdu/e7dh2pz6qWetkKSkp0qlTJ6lfv74MGzYsy55324mc/cuK7nmrF86Uk0fCxcvLW15q2V6aduiujkIO79sl//fNYvnr0nkp4l9CXnurr+opkpMN+ipCcjJfr9wyovU/VCgn3Xsgq36/IPN3nFb3vR5UUt59OUAK5c0jv5++KhM2HZeEmymSkx2b+OoTff+BM9bdHR+lTvmCDl2whhnLq1evLgMGDFDB/a9//UvGjh0rrVq1kjZt2kilSpXk3XfflZ9//lnmz58vmzdvlpIlS7p/WMOpU6dk//790qVLlyx7zpwe1mSssCZrTxrWB8/csPuxtcv/92S9Pf766y/59NNP5ZNPPlEzmgNCu2jRotK0aVN57733ZM+ePeZZzdHKDg4OloEDB+qjN0hAQIBaiIiyg6Y5dvSPxZKnp6dabPn7+8usWbPU/9EGjoiIUB0pxowZo0q9VatWNQc1IKgjIyPt3hZewUhEhqI5sCxYsECFquWCdZlp1KiRdO3aVdWu0aqOj49XYW7Jz89P4uLi7N5ul7esiYiylWb/Q0NCQqRXr15W69JrVduaM2eOJCQkqHr1pEmTJCkpKc334bZtq/1RGNZEZCiaA2mdUckjMzjJCMnJyTJkyBBp3769CmxLCGpvb/sHWWMZhIgMRXNS1z20pNHLw1LFihXl3r17UqxYMXW/7eNtSyOPwrAmIkPRHFgccfHiRdX748qVK+Z1UVFRUqRIEVXrPnbsmNy9+78rUcPDw6VmzZp2Pz/DmoiMRXNOWqP08eyzz8qIESPU1dk7d+6UadOmSb9+/aRu3bpSokQJGT58uMTExMjChQvlyJEj0qGD/TM4MayJyFA0J00+kCtXLpk3b574+PjIG2+8IaGhodKtWzfp3r27+T70CmnXrp1s2rRJ5s6da/cFMcATjERkKB5OHBqkePHiEhYWlu595cqVkxUrVjz2czOsichYNNElhjURGYqm07RmWBORoWj6zGqGNREZiyb6xLAmImPRRJcY1kRkKJpO05phTUSGoukzqxnWRGQsmugTw5qIDEXTadOaYU1EhqLpM6sZ1kRkLJroE8OaiIxFE11iWBORoWg6TWuGNREZiqbPrGZYE5GxaKJPDGsiMhZNdIlhTUSGouk0rTmtFxEZbqYYDzsXR2Ci3EGDBqn5Fhs0aCCTJk2S5ORkdd+FCxekZ8+eUqtWLWnRooXs3r3b8e12+DuIiHR+glGzc7FXamqqCuqkpCT5+uuv5bPPPpMdO3bIrFmz1H39+/eXokWLyvr166VNmzZqFvRLly45tN0sgxCRwWhZ/oynT5+WyMhI2bNnjwplQHhPmTJFXnzxRdWyXrVqleTNm1cCAgJk7969KrgHDhxo92swrInIUDQnlKyLFSsmixcvNge1ya1bt+Tw4cNStWpVFdQmwcHBKtwdwbAmIkPRHHhsSkqKWix5enqqxVKBAgVUndrk4cOHaibz5557TuLj48Xf39/q8X5+fhIXF+fQdrNmTUSGojlQs16wYIFqBVsuWJeZadOmSXR0tAwePFjVsW3DHbdtdwKZYcuaiAxFc6BtHRISIr169bJaZxu86QX18uXL1UnGSpUqiZeXlyQmJlo9BkHt7e3t0HYzrInIWDT7H5peyeNRJkyYICtXrlSB3bRpU7WuePHiEhsba/W4hISENKWRzLAMQkSGy2rNzsURYWFhqsfHzJkzpWXLlub1NWvWlGPHjsndu3fN68LDw9V6RzCsichQPDTN7sVep06dknnz5kmfPn1UXRsnFU0LLpIpUaKEDB8+XGJiYmThwoVy5MgR6dChg0PbzTIIERmLlvVPuW3bNnnw4IHMnz9fLZZOnjypgjw0NFTatWsn5cqVk7lz50rJkiUdeg0tFZfX5DDbTiS4ehMoGw36KsLVm0DZ6NjEV5/o+xNu3bf7sUV93ac96z5bQkSUDTR9juPEsCYiY9F0Ouoew5qIDEXTZ1azNwgRkR6wZU1EhqLptGXNsCYiQ9FYsyYicn+aPrOaYU1ExqIxrImI3J/GMggRkfvT9JnVDGsiMhZN9IlhTUTGookuMayJyFA0naY1w5qIDEXTZ1bnzCFSiYhyGo4NQkSkAwxrIiIdYFgTEekAw5qISAcY1kREOsCwJiLSAYY1EZEOMKyJiHSAYU1EpAMM6xwgOTlZRowYIbVr15YXXnhBlixZ4upNomyQkpIirVq1kn379rl6UygbcGyQHGDq1KkSFRUly5cvl0uXLsmwYcOkZMmS0qxZM1dvGjlxB/3hhx9KTEyMqzeFsgnDWufu3Lkja9eulUWLFsmzzz6rFvwBf/311wzrHCo2NlYFNYf1MRaWQXTuxIkTcv/+fQkMDDSvCw4OlsOHD8vDhw9dum3kHPv375d69erJ6tWrXb0plI3Ysta5+Ph4KVy4sHh6eprXFS1aVB0mJyYmSpEiRVy6fZT1unbt6upNIBdgy1rnkpKSrIIaTLdxAoqIcgaGtc55eXmlCWXTbW9vbxdtFRFlNYa1zhUvXlyuXbum6taWpREEdYECBVy6bUSUdRjWOlelShXJnTu3REZGmteFh4dL9erVxcODP16inIJ/zTrn4+Mjr7/+uowdO1aOHDkiP//8s7oopnv37q7eNCLKQuwNkgMMHz5chXWPHj3E19dXBg4cKK+++qqrN4uIshAnzCUi0gGWQYiIdIBhTUSkAwxrIiIdYFgTEekAw5qISAcY1kREOsCwJiLSAYY1EZEOMKwpyzVq1EgqV65sXjB7DWatWbZsWZa9Rrdu3eTzzz9X///444/VkhmMRrhmzZrHfs0NGzao90bkCrzcnJwCE/i2aNFC/R8jAv7+++8SGhoqhQoVUmOZZCU8rz1++OEH+eKLL6RTp05Z+vpE2YEta3KK/PnzS7FixdRSokQJadu2rdSvX19++uknp7wWlsxwZAXSM4Y1ZRsM5ZonTx5VwpgwYYI0btxYGjZsKLdu3ZLLly9Lv379pGbNmqrUEBYWJg8ePDB/79atW6Vp06ZSq1YtGT9+vNV9tmWQ7777TpVd8FydO3eW6Oho2bdvnxrw6s8//1SlmYsXL6rwnjt3rrzwwgtSu3Zt9fqYHd7kypUr8s4776jXxM7m/Pnz2fhpEVljWJPT3bt3T7Wo9+zZowLaVP+dNm2aCuV8+fLJgAEDxM/PTzZu3CiTJk2S77//XpUsTLN5f/DBB9KlSxdZv369KqtgzO707Nq1S5VFMALhpk2bpFq1ahISEqImFEZp5qmnnpLdu3er1v6KFSvU68yYMUNNPovXf/vtt9X2wvvvv68mHcbs8X369JHly5dn46dGZI01a3KKMWPGqNYz3L17V81cgwB97bXXVPihRR0UFKTu37t3r2rRYj0mTKhQoYIMGzZMtYT79++vAhot3549e6rHjxo1Snbs2JHu6yJ0W7VqpYIdhg4dqlrz169fV6WSXLlyqdIMLF68WG0nZgoHtNjRykbglylTRg4dOqRep2TJkvLMM89IVFSUbNmyJVs+PyJbDGtyikGDBpnH1MY8kQhIBKVJqVKlzP8/deqUmok9ODjYvA4tWoQ8pizD/ZgRxwTha3nb0pkzZ1Tpw3LyYAS/rdu3b0tcXJwMHjzYakYdvObZs2fV7PA4GYqgNsHsOwxrchWGNTkFSgrlypXL8H4EuAnKGmhNz5s3L83jTCcObU8OIrAzqovbw1Tznj17tpQvX97qvoIFC6rWvr2vSZQdWLMml0NYogxSpEgRFfBYcAJwzpw5ommaKkEcPXrUqtV94sSJdJ8L32t5H0IZJyxR48ZzmWAyYexQMLmw6TVRx0YdHa3zSpUqqdLJuXPnzN9z/Phxp30GRJlhWJPLoU6MsshHH30kJ0+elIMHD6q6NOaXROkE/aJRL54/f76cPn1apkyZYtVrwxJ6muDEIk5UImhxshItZFyYg+dDAKPMgdY8auCzZs2S7du3q3UjR46UiIgI1coPCAhQXQ1xUhLhj7ktcUKSyFUY1uRyCGQEMVrMCGbMIfnSSy+p8AS0enE/LmrBBTVoDeP+9NSpU0edNESXPJzMRGsYvUpwgvO5555Tz9W6dWu1vnfv3tKhQwcZPXq0el7sAL788ktVBoHPPvtMChcurGrgM2fOVDsCIlfhHIxERDrAljURkQ4wrImIdIBhTUSkAwxrIiIdYFgTEekAw5qISAcY1kREOsCwJiLSAYY1EZEOMKyJiHSAYU1EJO7v/wF5AfFS7QV4CQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#Prediction evaluation with top 50 positive and negative words\n",
    "def evaluate_final_k(X_bal, y_bal, pos_words, neg_words):\n",
    "    k = 50\n",
    "    print(f\"\\n=== Final model for predicting Willpower using top {k} positive + top {k} negative words ===\")\n",
    "\n",
    "    # Select the top 50 pos + 50 neg words\n",
    "    pos_k = pos_words[:k]\n",
    "    neg_k = neg_words[:k]\n",
    "    selected_features = pos_k + neg_k\n",
    "\n",
    "    # Ensure the selected features exist in TF-IDF columns\n",
    "    selected_features = [w for w in selected_features if w in X_bal.columns]\n",
    "    print(f\"Total usable features: {len(selected_features)}\")\n",
    "\n",
    "    # Subset TF-IDF matrix\n",
    "    X_sub = X_bal[selected_features]\n",
    "\n",
    "    # Train logistic regression on full dataset\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_sub, y_bal)\n",
    "\n",
    "    # Predict on training data (you can later swap to test set if needed)\n",
    "    y_pred = clf.predict(X_sub)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(y_bal, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_bal, y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(y_bal, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\",\n",
    "          metrics.classification_report(y_bal, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_bal, y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix (k = 50)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    return clf, selected_features, cm\n",
    "\n",
    "# Run final model\n",
    "clf_50, features_50, cm_50 = evaluate_final_k(\n",
    "    X_bal=X_bal,\n",
    "    y_bal=y_bal,\n",
    "    pos_words=pos_words,\n",
    "    neg_words=neg_words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f973d6cdf58c3f8",
   "metadata": {},
   "source": [
    "For Performance Self-talk and Effort, the results of logistic regression by using the reduced-feature are reported below.\n",
    "\n",
    "<img src=\"TF-IDF Assignment_files/images/top50_performance.png\" alt=\"Results for Performance Self-talk\" width=\"1960\">\n",
    "<img src=\"TF-IDF Assignment_files/images/reduced_model_Performance.png\" alt=\"Results for Performance Self-talk\" width=\"800\">\n",
    "<img src=\"TF-IDF Assignment_files/images/top50_effort.png\" alt=\"Results for Effort\" width=\"1960\">\n",
    "<img src=\"TF-IDF Assignment_files/images/reduced_model_Effort.png\" alt=\"Results for Effort\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5672cc89b606e5c2",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "The present study examined whether TF–IDF–based machine-learning models could predict human-coded motivational strategies from students’ open-ended responses. Overall, the results provide partial support for the hypotheses. Consistent with Hypothesis 1, the full TF–IDF models demonstrated strong predictive performance across all three motivational codes. For Willpower, the model achieved an accuracy of 0.88, with high precision (0.89) and recall (0.88). Similarly strong performance was observed for Performance Self-Talk (accuracy = 0.90, precision = 0.91, recall = 0.90) and Effort (accuracy = 0.89, precision = 0.90, recall = 0.89). These findings suggest that students’ qualitative descriptions contain identifiable lexical patterns that align with the conceptual structure of the human-coded motivational categories.\n",
    "\n",
    "The reduced-feature models also captured meaningful information but showed a consistent decrease in performance relative to the full models. When limited to the top 50 positive and top 50 negative predictors, accuracy dropped from 0.88 to 0.77 for Willpower, from 0.90 to 0.88 for Performance Self-Talk, and from 0.89 to 0.78 for Effort. Reductions were also observed in precision and recall. These results support Hypothesis 2, which predicted that reduced models would retain some predictive utility but perform less accurately than the full TF–IDF models. The decline in recall for the positive classes, particularly for Willpower (from 0.95 to 0.66) and Effort (from 0.98 to 0.66), suggests that the reduced models have difficulty capturing the full lexical variability present in students’ responses.\n",
    "\n",
    "Finally, the findings provide support for Hypothesis 3, which proposed that TF–IDF–based NLP models would demonstrate above-chance ability to predict human-coded motivational regulation strategies, while the most predictive lexical features would not align cleanly with the conceptual categories used by human coders. Across all three motivational codes, the TF–IDF models achieved accuracies far above chance-level, indicating that students’ open-ended reflections contain reliable linguistic signals associated with human-coded motivational strategies. However, consistent with the second part of the hypothesis, manual inspection of the top positive and negative TF–IDF predictors revealed that many of the highest-weighted terms did not map neatly onto the theoretical definitions guiding the qualitative coding. Several predictive words were either overly general (e.g., “work,” “try,” “thing”), highly context-dependent, or completely irrelevant. This divergence suggests that while the models can learn statistically discriminative features, these features do not necessarily capture the conceptual nuance emphasized in human coding rubrics.\n",
    "\n",
    "The current study demonstrates the potential of NLP methods as complementary tools for motivational research. Automated models could support qualitative analyses at scale in educational settings, enabling instructors or learning scientists to efficiently monitor changes in students’ motivational strategies over time. In addition, identifying the most predictive lexical features may inform the design of real-time learning analytics dashboards or adaptive feedback systems.\n",
    "\n",
    "Several additional limitations should be noted. First, this study lacked a robust methodological approach to handle the complexity and noise of qualitative input. Although two procedures were implemented to detect and remove nonsensical or low-quality strings, manual inspection revealed that a small portion of unusable or incoherent responses remained in the final dataset. These noisy inputs may have introduced unpredictable variance into the TF–IDF feature matrix. Second, the dataset itself poses limitations. The original distribution of human-coded categories was highly imbalanced, with the “1” category substantially less frequent than the “0” category across all three motivational strategies. To mitigate the bias this imbalance could introduce, the present study relied on random sampling to obtain a balanced subset for all machine-learning analyses. While this approach improves class balance, it also limits generalizability, as the resulting performance metrics do not reflect how the model would perform under naturally occurring class ratios. Third, the TF–IDF representation introduces conceptual constraints. TF–IDF treats language as isolated tokens and does not capture context, syntax, negation, or the semantic structure of motivational strategy reflections. As a result, some of the most predictive features may reflect superficial lexical regularities rather than psychologically meaningful indicators of motivational regulation. Fourth, the machine-learning procedure used in this study limits the interpretability and external validity of the findings. Because the models were evaluated on the same balanced dataset used for training, the reported accuracy, precision, and recall values likely overestimate actual generalization performance. Without a separate test set or cross-validation framework, the results should be interpreted as demonstrating the model’s sample-specific discriminative ability rather than its predictive robustness in new or unseen data.\n",
    "\n",
    "Future work can build on this study in several directions. First, addressing the limitations of TF–IDF by adopting contextualized language models such as BERT can substantially improve both accuracy and conceptual alignment. Unlike TF–IDF, which treats words as independent tokens, contextualized models capture semantic relationships, syntactic structure, and pragmatic meaning. This would allow the classifier to detect more nuanced motivational expressions, which are often indistinguishable under a bag-of-words representation. Second, future research should consider dimensionality-reduction approaches to construct more conceptually coherent representations of motivational strategies. Techniques such as principal component analysis (PCA) can reduce feature space sparsity while yielding interpretable latent dimensions. A more compact representation of motivational language would reduce reliance on manual dataset balancing, as the model would operate on aggregated, semantically enriched dimensions rather than sparse individual words. This approach may also help preserve the natural distribution of categories rather than requiring artificial subsampling. Third, future work should implement rigorous model-evaluation procedures, such as cross-validation or separate hold-out test sets. This would provide a more accurate estimate of generalization performance and reduce the risk of overfitting that arises when training and testing occur on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90505d5b42b5344a",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\">References</h3>\n",
    "\n",
    "Gamieldien, Y., Case, J. M., & Katz, A. (2023). Advancing qualitative analysis: An exploration of the potential of generative AI and NLP in thematic coding. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.4487768\n",
    "\n",
    "Hucka, M. (2018). Nostril: A nonsense string evaluator written in Python. Journal of Open Source Software, 3(25), 596, https://doi.org/10.21105/joss.00596\n",
    "\n",
    "Miele, B.D., Zepeda, C.D., Kim, S., & Scholer, A.A. (in prep). Students’ Self-Regulation of Their Motivation.\n",
    "\n",
    "Monette, M., Peroutka, M., Richey, J.E., & Zepeda, C.D. (in press). Beyond Hypotheticals: Re-examining Authentic Motivational Regulation Strategies in Exam Preparation. Journal of Educational Psychology.\n",
    "\n",
    "Neuhaus, R. & Ruvinskiy, R. (2015). Gibberish-Detector [Source code]. GitHub. https://github.com/rrenaud/Gibberish-Detector\n",
    "\n",
    "Schwinger, M., Steinmayr, R., & Spinath, B. (2009). How do motivational regulation strategies affect achievement: Mediated by effort management and moderated by intelligence. Learning and Individual Differences, 19(4), 621–627.\n",
    "\n",
    "Wolters, C. A. (1999). The relation between high school students' motivational regulation and their use of learning strategies, effort, and classroom performance. Learning and Individual Differences, 11(3), 281–299. https://doi.org/10.1016/S1041-6080(99)80004-1\n",
    "\n",
    "Zimmerman, B. J., & Schunk, D. H. (2012). Motivation: An essential dimension of self-regulated learning. In Motivation and self-regulated learning (pp. 1-30). Routledge.\n",
    "\n",
    "*Acknowledgement*: This work was assisted by generative AI (ChatGPT) and Grammarly for polishing grammar and refining the language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
